{
  
    
        "post0": {
            "title": "HIDA-Datathon",
            "content": "HIDA Datathon on Climate Change .",
            "url": "https://srsteinkamp.github.io/srspage/hida/2020/12/19/HIDA-hackathon.html",
            "relUrl": "/hida/2020/12/19/HIDA-hackathon.html",
            "date": " • Dec 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "30th place solution in Kaggle",
            "content": "30th place solution in Kaggle . This post is also available in the Kaggle forums. . First of all thanks to the organizers to putting together this challenging and interesting competition! . I’ve seen many good and well written and well developed solutions on the challenge … mine is neither. But I just wanted chip in, as I might have used some feature engineering that hasn’t been used as widely. Unfortunately I don’t have the time right now to dig into it in detail, but will just give you a rough description about what I did. . Reflections . A personal note on how I perceived the challenge. I was really exciting to see this challenge, this should lie in my area of expertise, so why not give it a try. So I did, wrote a for my terms successful EDA notebook, got into the data and somehow got to 13th place. But, as people noted, there was not much going on in terms of discussion etc., so I might have gotten a bit complacent with my position and did not develop much further. Also I might have gotten a bit annoyed, because I couldn’t get my errors on domain1_var1 and the domain2 vars done as much as I liked. . Then life happened, vacation, deadlines, an online hackathon. Not much space for the challenge, and suddenly there are 10 days left, I have a deadline coming up, and find myself around place 50th on the leaderboard. So some panic sets in and also some craziness, so I rewrote the analysis, and preprocessing from scratch, run all the different models. Stack them up and well suddenly it’s place 34, and 30 on the private leaderboard - my best competition so far. . Preprocessing and Features . Targets . I decided to perform stratified cross-validation on my data, so I used KNN(n_clusters=15) on the target variables, setting the missing values to 0, so that this will be taken into account for the stratification approach. | Then I imputed the missing data using sklearn.impute.IterativeImputer to get values there. This seemed to be a good balance for me between discarding the data or just filling in a constant. However, I didn’t really look into the imputations afterwards… | Preprocessing and Feature Engineering . loading.csv - Discarded IC_20 - Seemed to be very different between train and test (discussions and kernels) | fnc.csv - Reordering the data for use with nilearn.connectome.vec_to_sym_matrix | Extracting data from the 4D feature maps. I have a kernel out-there showing how this can be done using different Masker objects using nilearn.input_data. I extracted the sets schaeffer (400 brain regions), msdl, and basc (197 regions) here. | Graph-based features: Several publications also focus on using graph-theory based descriptions of functional connectivity as features for machine learning classifiers. So I used the “Brain Connectivity Toolbox”, bct.py to derive features. These were just a ton of global and local descriptions, also at different thresholdings of the connectivity matrices (derived from fnc). Around 1700 features in total (with sometimes really bad numerical issues), which I somehow hacked away in the preprocessing before my stacking data. This will be named fnc_graph. | Combined data, I also created the set loadfnc, combining fnc and loading data, and loadmsdl combining msdl and loading. Finally, I also used a loadnoise set, where I added some random intercept and Gaussian noise to the loading data (differently for each subset). If that helped at all, I couldn’t unfortunately test. | . Stacking, stacking, stacking . I stacked tons of models (32 for each feature) using different regression approaches, and sometimes different preprocessing. ConnectivityMeasure (shortened to CM). is a class from nilearn.connectome, that can be used to transform a n x t matrix in to a n x n connectivity matrix, using differnt kinds of connectivity. The nice thing is, it also fits into sklearn-pipelines as a vectorized version of the matrix is possible. . Data Set Preprocessing SVR, LGBM Preprocessing Regression . basc | CM(tangent), PCA(whiten=True), RobustScaler() | CM(correlation), PCA(whiten=True) | . msdl | CM(tangent), PCA(whiten=True), RobustScaler() | CM(correlation), PCA(whiten=True) | . schaeffer | CM(tangent), PCA(whiten=True), RobustScaler() | CM(correlation), PCA(whiten=True) | . fnc | PCA(whiten=True) | abs(fnc) &lt; 0.15 = 0, PCA(whiten=True) | . fnc_pca | None | abs(fnc) &lt; 0.15 = 0, None | . loading | RobustScaler() | RobustScaler() | . fnc_graph | Numerical fixes, PCA(whiten=True), RobustScaler() | Numerical fixes, PCA(whiten=True), RobustScaler() | . loadmsdl | PCA(whiten=True), RobustScaler() | PCA(whiten=True), RobustScaler() | . loadfnc | RobustScaler() | RoubstScaler() | . loadnoise | RobustScaler() | RobustScaler() | . SVR . I used sklearns SVR using both a linear and a rbf kernel on the datasets basc, msdl, schaeffer, fnc_pca, loading, and fnc_graph. To figure out the best parameters I applied skopt.BayesianSearchCV with 35 iterations (objective mean absolute error). Parameters optimized where C, epsilon and n_components for PCAs. . So here are 2 x 6 = 12 models. . Regression . In this competition I somehow came to like the LassoLars regression of sklearn. So that’s what I am using here. The feature selection of it seemed to help actually. Running models on basc, msdl, schaeffer, fnc, loading, fnc_graph, loadmsdl, loadfnc, and loadnoise. This time optimizing mean squared error using BayesianSearchCV for alpha and n_components. . So 9 models. . LightGBM . Same datasets as for Regression. Optimizing tree parameters and PCA, best model defined by mean absolute error. . Another 9 models. . 2D CNN . I also tried to get some more spatial information into the model as well so I set up a small 2D CNN having: . Conv2D Layer, with ReLU activation | Maxpooling (2, 2) | Flatten | Dropout() | Dense(1) | . Where the number of filters, kernel_size, Dropout, learning_rate, and the loss (mae, mse), where found through BayesianSearchCV. . Training and Prediction . I used the same approach for all models and the final stacking model: . Optimize hyperparameters on 5-Folds. | Retrain model on CV-Data | Evaluate on hold-out set | Retrain on all data | Predict on test set. | The final stacking model was again a LassoLars regression, on the outputs of the 31 models. I actually preprocessed the predictions, by slapping a RobustScaler in just for good measure. . Thoughts . I learned quite a lot from the competition, but have to say that I am not really satisfied with what I did (my best competition so far…), and see a lot of room for improvement. . Work smarter . I think the most annoying part for me is, that I just stacked tons of models. In the end not even thinking much about why I am doing it. I just wanted to get that 0.001 Leaderboard boost, to get a little edge. But, if I had invested my time more into careful tuning, preprocessing, and careful model selection, I think I would have gotten more out in the last weekend of the competition, than I did here. In the end, I was mostly waiting for models to finish running and to start the next set of long calculations. . Evaluate, evaluate, evaluate . So far, my intuition on evaluating locally and avoiding overfitting got me quite far (I got my first silver medal basically because of an incredibly heavy shake up of the leaderboard, pushing me a couple of 100 places or so to the top). Here I think I was actually quite lucky - see the forum posts where people discuss about the lack of a shake up. . Get a team . Next time I am in this situation, I think I will team up. Also I apologize to the people who contacted me and I didn’t get back to. Here it was mostly bad timing, but I think there is some much to gain in terms of insights, when you can discuss your solutions :) . And of course much more. .",
            "url": "https://srsteinkamp.github.io/srspage/python/kaggle/2020/07/03/The-30th-Place-Solution.html",
            "relUrl": "/python/kaggle/2020/07/03/The-30th-Place-Solution.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Reaction Time Analysis: The exponential Gaussian",
            "content": "The (not so) easy task of analysing reaction times. . This is the non-interactive Version of the analysis! You can find the interactive notebook here, caution it’s about 7MB in size, so not your average website! . All code can be found in rmarkdown documents on Github https://github.com/SRSteinkamp/ReactionTimeWrangling/exgauss . Analyzing reaction times has a long history in psychology and (cognitive) neuroscience. Most people who studied Psychology in university have conducted a classical psychological experiments. For example investigating the Stroop task. You might have encountered the stimuli, with the typical task to spell out loud as fast as possible written color names: . $ color{red}{ text{RED}}$ | $ color{yellow}{ text{BLUE}}$ | $ color{blue}{ text{YELLOW}}$ | $ color{green}{ text{GREEN}}$ | . After running such a task, with many more word-color pairs and different participants, a typical question in undergrad might be: Are participants significantly faster while reading matching color words ($ color{green}{ text{GREEN}}$) than reading non-matching color words ($ color{blue}{ text{GREEN}}$)? A straightforward answer would be to to average the reaction times in the non-matching and matching conditions for each participant and then compare the two conditions using for example a paired t-test. Yes, if p &lt; 0.05 and no, if p &gt; 0.05. . Results are in and everything is fine? If you start wondering, whether this is the correct way of analysis, you might find more and more and is more. Different discussions about: . Should conditions across participants be averaged using the mean or the median? | Should data be averaged at all? | How to define outliers? | … | Could a drift-diffusion model provide the key insights? | … | Is null hypothesis significance testing meaningful? | . All of these questions are not really in my main field of expertise (no worries I won’t deal with the last one ;) , but I found reading about reaction time analyses weirdly entertaining and very interesting, and I wanted to start with blogging. So here is the first one of a couple of experiments I am planning to do. There is no particular order but all are based on some questions that arose while looking at different papers. . So maybe there is something useful here for you, or not. Or you disagree or have comments, suggestions, etc. please get in touch! . Here is the first part: . Part 1 - How many trials do I need to fit an Ex-Gauss? . Data below is generated sampling 100000 observations from two Ex-Gaussian distributions with μ = 300, σ = 20, τ = 300 (red) and μ = 500, σ = 50, τ = 100. Note, that both distributions have the same mean of 600 (the black dashed line). To the left the two components of the Ex-Gaussian are presented - the Gaussian and the Exponential distributions. Here again the dashed-lines describe the mean of the corresponding distributions. . The Ex(ponential) Gauss(ian) distribution, is the sum of a Gaussian distribution parametrized by μ, and σ, which define the “body” of the distribution, with an Exponential function (τ) describing the skew to the right. The normally distributed body, with a long tail, has been found to closely match the distribution of reaction time data found in many experiments (Palmer et al. 2011). While the fit to experimental data seems to be ideal, the parameters itself do not seem to be related to any specific cognitive constructs. At least, the discussion is still ongoing (Spieler, Balota, and Faust 2000). The strength of fitting distributions to reaction times is seen in the ability to provide a finer description than for example a summary of a certain condition using the mean or the median. Different combinations of the Ex-Gauss parameters μ and τ, for example can lead to the same mean. So comparing two conditions for example might provide the same summary statistics, but the distributions might have a very different spread and skew. . If you are interested in checking out more distributions (and more about model fitting, etc.) visit this great page: https://lindeloev.github.io/shiny-rt/ . This is just a quick introduction into why fitting a distribution might provide a better picture of reaction times, but how many trials are necessary per condition? . Methods . To simulate data I used the 12 Ex-Gauss distributions used by Miller (1988), and many others. My assumption is, that a researcher wants to fit an Ex-Gauss function for each condition and each participant in an experiment. Note that, there are ways to estimate distributions across multiple participants, which seem to be stable, even for small numbers of trials(Ratcliff 1979), which I am not (yet?) going into. One estimate is that around 100 trials might be needed to get reliable results (Ratcliff 1979). . Here I want to investigate how many trials are needed, and whether there are general biases in the estimation. For most of the analysis I am using the retimes package. Data is simulated using rexgauss and for model fitting using both the method of moments (mexgauss) and maximum likelihood estimation (MLE) timefit are used. According to the documentation timefit gets its starting parameters using the method of moments. . I am simulating data from the 12 distributions starting with 10 (maybe a rare-condition, like an oddball), up to 500 trials (a Psychophysicist’s dream (Palmer et al. 2011)). For each of the twelve distributions I sampled different numbers of trials (10, 20, 35, 50, 100, 200, 350, 500) and then estimated the three parameters μ, σ, τ using the method of moments and MLE. This processes was repeated 10000 times. . Distribution Mu Sigma Tau . 1 | 300 | 20 | 300 | . 2 | 300 | 50 | 300 | . 3 | 350 | 20 | 250 | . 4 | 350 | 50 | 250 | . 5 | 400 | 20 | 200 | . 6 | 400 | 50 | 200 | . 7 | 450 | 20 | 150 | . 8 | 450 | 50 | 150 | . 9 | 500 | 20 | 100 | . 10 | 500 | 50 | 100 | . 11 | 550 | 20 | 50 | . 12 | 550 | 50 | 50 | . Results . The histograms below describe our simulation results. Feel free to click around (in the interactive version) and select different distributions and methods. From our visual inspection we can see that using a small number of trials can lead to quite some biases in the estimated parameters. The spread of estimated parameters decreases the more samples are considered. The data in the histograms is filtered to only include parameter estimates greater than 0 and less than 750. The histograms are calculated so that 40 bins for each sample are estimated and only for sample sizes of 20, 100, 500). This might not be the best way to display the data, but was done to keep the size of the HTML as small as possible. Furthermore, some data cleaning had to be performed as there are raw-events with very unlikely parameter estimates. In the non-interactive version, there estimates for distribution 6 only, however you can see Moments and MLE side by side. . Interactive Version Legend . I couldn’t figure out how to put meaningful legends on the interactive version: * red estimates for μ, solid bars distribution μ * blue estimates for τ, dotted bars distribution τ * green estimates for σ, dashed bars distribution σ To get an estimate of how well (or bad) the modeling performed, I calculated the mean error to investigate general trends, its standard-deviation (SD), and the mean absolute error for error estimation (MAE). . There is unfortunately, too much data to have good look at, so here is a datatable to play around with and investigate some of the summary values (in the interactive version only). Data can be created using the .Rmd files in the Repro . Analysis . Plotting the summaries for the estimations and pooling over distributions, we can draw first (maybe obvious) conclusions: . larger sample sizes, lead to less error | maximum likelihood estimation performs generally better, than the method of moments. | . Interestingly, regardless of estimation procedure, the σ and μ parameters seem to be overestimated, whereas τ is underestimated. . . Statistical Summary . How strong are the observed biases? . As we have already seen in the figure, we confirm that μ is generally overestimated while, τ is underestimated. This makes sense given the distribution of the data: The majority of data will be sampled from the Gaussian part of the distribution, so extreme-values are relatively rare. It is therefore much harder to correctly estimate the skew (τ). As the mean of the Ex-Gaussian is given by μ + τ, a underestimation of τ automatically leads to a larger estimate of μ. . Also, the method of moments seems to be more prone to biases, especially leading to an overestimation of σ. . What can we learn about the error? . . All factors which were included in the model appear to have some meaning (are significant). As we have seen in the other analysis, the method of moments has a higher base MAE rate than the maximum likelihood estimation. Furthermore, we can see that μ and τ are easier identifiable the farther they are apart, especially when the method of moments is used. This is expressed by the regressor diff_mt = μ − τ (based on the original distributions). And again: larger sample sizes are the key factor to reduce the error! . Conclusion . If you want to get a good estimate of the reaction time distribution: collect enough data! . To provide a bit more nuance, you can get away with small sample sizes, if you are lucky. For example if the reaction time distribution isn’t very skewed. But keep in mind, that you can expect a higher error in the estimation of the true parameters (especially using the method of moments), when τ and μ are close to each other. Looking at the histograms, we also see that there is a lot of variance in the estimation of σ. So further analyzing σ, for example in a group analysis, should be done very carefully. Last but not least, sample sizes should be equal when comparing μ and τ across different conditions! Even if parameters are drawn from the same Ex-Gauss distribution, it is very likely that the condition with less trials will have a higher estimate of these parameters. We can do a small simulation of this using our simulated data. . Type 1 error due to imbalanced sample sizes . For simplicity I decided to only use samples from distribution 6 (with μ = 400, τ = 200, and σ = 50), estimated by MLE. I am drawing 30 random sets of estimated parameters for different combinations of sample sizes. The number 30 is quite arbitrary but is supposed to reflect a typical number of participants in an reaction time experiment. The parameters of the different distributions are then submitted to a paired two-sided t-test and the number of significant results (p &lt; 0.05) are reported. In theory, as data is drawn from the same distribution, we should expect around 5% false positive results. . Histogram of the simulation results. We can see that the first two pairs have many false positives (p &lt; 0.05, left of the black line). The later pairings on the other hand seem to have a rather uniform distribution of p-values (as we would expect). The pairings (e.g., 20 : 50) show on how many trials the parameters in condition 1 (20) and in condition 2 (50) were estimated. The paired t-tests were then calculated as condition 1 &gt; condition 2. . For completeness sake I also calculated the average t-value for each of the pairing, next to the proportion of false positive results. . Pairing Mu_p Mu_t Tau_p Tau_t . 1 - 20 : 50 | 0.248 | 1.406 | 0.166 | -1.065 | . 2 - 20 : 200 | 0.378 | 1.762 | 0.256 | -1.407 | . 3 - 50 : 100 | 0.053 | 0.289 | 0.057 | -0.293 | . 4 - 50 : 200 | 0.060 | 0.434 | 0.068 | -0.408 | . 5 - 100 : 200 | 0.049 | 0.142 | 0.056 | -0.092 | . 6 - 200 : 200 | 0.050 | -0.009 | 0.051 | 0.005 | . As assumed, we have a inflation of false-positive t-tests when comparing estimates of Ex-Gauss parameters from the same distribution (but estimated using different sample-sizes). The larger the imbalance, the larger the false positive rate! . Conclusions not related to analysis . This is the first larger project I conducted in R(markdown). I really don’t like the basic R syntax in many regards, but using dplyr and the rest of the tidyverse is great :) %&gt;% it all the way! Building interactive figures with ggplot2 + plotly + crosstalk is also quite amazing. But as I did not want to create a shiny app, figuring out how to deal with exploding sizes of .html files took me quite some time. . References . Miller, Jeff. 1988. “A Warning About Median Reaction Time.” Journal of Experimental Psychology: Human Perception and Performance 14 (3): 539–43. https://doi.org/10.1037/0096-1523.14.3.539. . Palmer, Evan M., Todd S. Horowitz, Antonio Torralba, and Jeremy M. Wolfe. 2011. “What Are the Shapes of Response Time Distributions in Visual Search?” Journal of Experimental Psychology: Human Perception and Performance 37 (1): 58–71. https://doi.org/10.1037/a0020747. . Ratcliff, Roger. 1979. “Group Reaction Time Distributions and an Analysis of Distribution Statistics.” Psychological Bulletin 86 (3): 446–61. https://doi.org/10.1037/0033-2909.86.3.446. . Spieler, Daniel H., David A. Balota, and Mark E. Faust. 2000. “Levels of Selective Attention Revealed Through Analyses of Response Time Distributions.” Journal of Experimental Psychology: Human Perception and Performance 26 (2): 506–26. https://doi.org/10.1037/0096-1523.26.2.506. .",
            "url": "https://srsteinkamp.github.io/srspage/reaction%20times/modeling/r/2020/04/30/Reaction-Time-Analysis-Ex-Gauss.html",
            "relUrl": "/reaction%20times/modeling/r/2020/04/30/Reaction-Time-Analysis-Ex-Gauss.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "HIDA-Datathon in April",
            "content": "02-03 April 2020 HIDA-datathon . . This remote/online 36h datathon was organized by the Helmholtz Information &amp; Data Science Academy, inplace of the big datathon: Grand Challenges on Climate Change, which couldn’t take place due to the coronavirus. . This was a very interesting experience, participating in a challenge to analyse and find structure in previously unknown dataformats (especially map information) and fields (geo/climate science). After the challenge was stated, we realized that there were a couple of teams who had really concrete ideas as how to analyse the data and what kind of computational models to use. . Our team agnostic decided that not assuming anything in particular would be the best approach, and using our very different background and levels of expertise, we were able to put together a good summary of the data at hand. Deciding to throw many conventions out of the window, we also found out some things that were surprising to the domain experts, for example that longitudinal data (even in summary chunks) provides valuable information for modeling aspects. .",
            "url": "https://srsteinkamp.github.io/srspage/2020/04/30/HIDA-datathon.html",
            "relUrl": "/2020/04/30/HIDA-datathon.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Pandas suprise - NaNs and with groupby",
            "content": "import pandas as pd import numpy as np from pandas.errors import UnsupportedFunctionCall . I figured out something about pandas today, which I was very surprised by. Applying .groupby on a pd.DataFrame automatically ignores NaN values. This is intendet behavior, but sometimes you actually want to have some NaN in the data, to check whether your data-frame is correct and to find possible corruptions. . Here is a little example: . DF = pd.DataFrame.from_dict({&#39;g1&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;g2&#39;: [&#39;c&#39;, &#39;c&#39;, &#39;d&#39;, &#39;d&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;, &#39;d&#39;], &#39;d1&#39;: [0, 1, np.nan, 3, 4, 5, 6, 7]}) . Averaging the entries in DF, we would expect a NaN in group a, d, but we get 3.0! . DF.groupby([&#39;g1&#39;, &#39;g2&#39;]).mean() . d1 . g1 g2 . a c 0.5 | . d 3.0 | . b c 4.5 | . d 6.5 | . If you apply pandas .mean() method on a DataFrame you could speciy a skipna = False in the function. This, unfortunately doesn&#39;t work after using .groupby. . try: DF.groupby([&#39;g1&#39;, &#39;g2&#39;]).mean(skipna=False) except UnsupportedFunctionCall: print(&#39;UnsupportedFunctionCall&#39;) . UnsupportedFunctionCall . I think, I have seen one solution to solve this issue statingt that using .apply(np.mean) instead of using .mean() might solve the problem. . However: . DF.groupby([&#39;g1&#39;, &#39;g2&#39;]).apply(np.mean) . d1 . g1 g2 . a c 0.5 | . d 3.0 | . b c 4.5 | . d 6.5 | . Calling np.mean causes pandas to bypass the function and calls DF.mean() from pandas with skipna=True! As far as I know, you have to create a new function to solve the issue. . def mean_w_nan(x): # Don&#39;t forget the np.array call! return np.mean(np.array(x)) DF.groupby([&#39;g1&#39;, &#39;g2&#39;]).apply(mean_w_nan) . g1 g2 a c 0.5 d NaN b c 4.5 d 6.5 dtype: float64 . References: . https://stackoverflow.com/questions/26145585/pandas-aggregation-ignoring-nans | https://github.com/pandas-dev/pandas/issues/15674 | https://stackoverflow.com/questions/54106112/pandas-groupby-mean-not-ignoring-nans | .",
            "url": "https://srsteinkamp.github.io/srspage/2020/04/28/Pandas-GroupBy-NaN.html",
            "relUrl": "/2020/04/28/Pandas-GroupBy-NaN.html",
            "date": " • Apr 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://srsteinkamp.github.io/srspage/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://srsteinkamp.github.io/srspage/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}